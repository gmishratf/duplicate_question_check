{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from textstat.textstat import textstat\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START_INDEX = 0\n",
    "END_INDEX = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['avg_word_length_diff', 'avg_word_length_diff_with_spaces',\n",
    "                'sentence_length_diff_with_spaces', 'dup_words_diff', 'oov_words_diff', \n",
    "                'syllable_count_diff', 'lexicon_count_diff', 'min_kcore', 'max_kcore', \n",
    "                'similar_neighbors', 'similar_neighbor_ratio', 'min_freq', 'max_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_df = pd.DataFrame(columns=feature_list)\n",
    "test_feature_df = pd.DataFrame(columns=feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = pd.read_csv('./Data/train_cleaned.csv')\n",
    "train_dataset.fillna('NO QUESTION', inplace=True)\n",
    "test_dataset = pd.read_csv('./Data/test_cleaned.csv')\n",
    "test_dataset.fillna('NO QUESTION', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./Data/train_cleaned.csv')\n",
    "train_df.fillna('NO QUESTION', inplace=True)\n",
    "test_df = pd.read_csv('./Data/test_cleaned.csv')\n",
    "test_df.fillna('NO QUESTION', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = train_dataset[START_INDEX:END_INDEX]\n",
    "test_df = test_dataset[START_INDEX:END_INDEX]\n",
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = \"\"\n",
    "TYPE_DATA = \"\"\n",
    "train_ques1 = train_df['question1']\n",
    "train_ques2 = train_df['question2']\n",
    "test_ques1 = test_df['question1']\n",
    "test_ques2 = test_df['question2']\n",
    "NUMBER_OF_FEATURES = len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_word_length_diff(q1, q2):\n",
    "    avg1 = (len(q1)-q1.count(' '))/(1.0 * len(q1.split()))\n",
    "    avg2 = (len(q2)-q2.count(' '))/(1.0 * len(q2.split()))\n",
    "    return avg1-avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_word_length_diff_with_spaces(q1, q2):\n",
    "    avg1 = len(q1)/(1.0 * len(q1.split()))\n",
    "    avg2 = len(q2)/(1.0 * len(q2.split()))\n",
    "    return avg1-avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length_diff(q1, q2):\n",
    "    return len(q1)-len(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_duplicate_words_diff(q1, q2):\n",
    "    counts1 = Counter(q1.split())\n",
    "    counts2 = Counter(q2.split())\n",
    "    sum1 = sum2 = 0\n",
    "    for word, count in counts1.items():\n",
    "        if count>1:\n",
    "            sum1 += sum1\n",
    "    for word, count in counts2.items():\n",
    "        if count>1:\n",
    "            sum2 += sum2\n",
    "    return sum1-sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dup_words_diff(q1, q2):\n",
    "    words1 = q1.split()\n",
    "    words2 = q2.split()\n",
    "    fd1 = nltk.FreqDist(words1)\n",
    "    fd2 = nltk.FreqDist(words2)\n",
    "    nd1 = nd2 = 0.0\n",
    "    for w, f in fd1.items():\n",
    "        if f>1:\n",
    "            nd1 = nd1 + 1\n",
    "    nd1 = nd1/(1.0*len(words1))\n",
    "    for w, f in fd2.items():\n",
    "        if f>1:\n",
    "            nd2 = nd2 + 1\n",
    "    nd2 = nd2/(1.0*len(words2))\n",
    "    return nd1-nd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oov_words_diff(q1, q2):\n",
    "    return textstat.difficult_words(q1) - textstat.difficult_words(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def syllable_count_diff(q1, q2):\n",
    "    return textstat.syllable_count(q1) - textstat.syllable_count(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexicon_count_diff(q1, q2):\n",
    "    return textstat.lexicon_count(q1) - textstat.lexicon_count(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hash(train_df, test_df):\n",
    "    train_q = np.dstack([train_df['question1'], train_df['question2']]).flatten()\n",
    "    test_q = np.dstack([test_df['question1'], test_df['question2']]).flatten()\n",
    "    q_complete = np.append(train_q, test_q)\n",
    "    q_complete = pd.DataFrame(q_complete)[0].drop_duplicates()\n",
    "    q_complete.reset_index(inplace=True, drop=True)\n",
    "    q_dict = pd.Series(q_complete.index.values, index=q_complete.values).to_dict()\n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hash(df, dict_hash):\n",
    "    df['qid1'] = df['question1'].map(dict_hash)\n",
    "    df['qid2'] = df['question2'].map(dict_hash)\n",
    "    return df.drop(['question1', 'question2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_hash(df):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(df.qid1)\n",
    "    edges = list(df[['qid1', 'qid2']].to_records(index=False))\n",
    "    graph.add_edges_from(edges)\n",
    "    graph.remove_edges_from(graph.selfloop_edges())\n",
    "    op_df = pd.DataFrame(data=graph.nodes(), columns=['qid'])\n",
    "    op_df['kcore'] = 0\n",
    "    for i in tqdm(range(2, 11), desc='CALCULATING KCORE FEATURES'):                  #2nd argument in range is number of k-core features\n",
    "        ck = nx.k_core(graph, k=i).nodes()\n",
    "        print(\"kcore\", i)\n",
    "        op_df.loc[op_df.qid.isin(ck), 'kcore'] = i\n",
    "        #print(op_df[:5])\n",
    "    return op_df.to_dict()['kcore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kcore_from_dict(df, k_dict):\n",
    "    df['k_q1'] = df['qid1'].apply(lambda x: k_dict[x])\n",
    "    df['k_q2'] = df['qid2'].apply(lambda x: k_dict[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_minmax(df, col):\n",
    "    sorted_features = np.sort(np.vstack([df[col + \"1\"], df[col + \"2\"]]).T)\n",
    "    df['min_' + col] = sorted_features[:, 0]\n",
    "    df['max_' + col] = sorted_features[:, 1]\n",
    "    return df.drop([col + \"1\", col + \"2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neighbors(train_df, test_df):\n",
    "    neighbors = defaultdict(set)\n",
    "    for i in [train_df, test_df]:\n",
    "        for q1, q2 in zip(i['qid1'], i['qid2']):\n",
    "            neighbors[q1].add(q2)\n",
    "            neighbors[q2].add(q1)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbor_similarity(df, neighbors):\n",
    "    common = df.apply(lambda x: len(neighbors[x.qid1].intersection(neighbors[x.qid2])), axis=1)\n",
    "    min_n = df.apply(lambda x: min(len(neighbors[x.qid1]), len(neighbors[x.qid2])), axis=1)\n",
    "    df['similar_neighbor_ratio'] = common/min_n\n",
    "    df['similar_neighbors'] = common.apply(lambda x: min(x, 5)) #2nd argument is upper bound for number of neighbors\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_features(df, freq_map):\n",
    "    df['freq1'] = df['qid1'].map(lambda x: min(freq_map[x], 100))  #2nd argument in min is upper bound for frequencies\n",
    "    df['freq2'] = df['qid2'].map(lambda x: min(freq_map[x], 100))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE GENERATION DRIVER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------CALCULATING FEATURES------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------STATISTICAL FEATURES------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(START_INDEX, END_INDEX), desc='CREATING STATISTICAL FEATURES FOR TRAIN'):\n",
    "    feature = np.empty(NUMBER_OF_FEATURES) * np.nan\n",
    "    feature[0] = avg_word_length_diff(train_ques1[i], train_ques2[i])\n",
    "    feature[1] = avg_word_length_diff_with_spaces(train_ques1[i], train_ques2[i])\n",
    "    feature[2] = sentence_length_diff(train_ques1[i], train_ques2[i])\n",
    "    feature[3] = dup_words_diff(train_ques1[i], train_ques2[i])\n",
    "    feature[4] = oov_words_diff(train_ques1[i], train_ques2[i])\n",
    "    feature[5] = syllable_count_diff(train_ques1[i], train_ques2[i])\n",
    "    feature[6] = lexicon_count_diff(train_ques1[i], train_ques2[i])\n",
    "    train_feature_df.loc[len(train_feature_df)] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(START_INDEX, END_INDEX), desc='CREATING STATISTICAL FEATURES FOR TEST'):\n",
    "    feature = np.empty(NUMBER_OF_FEATURES) * np.nan\n",
    "    feature[0] = avg_word_length_diff(test_ques1[i], test_ques2[i])\n",
    "    feature[1] = avg_word_length_diff_with_spaces(test_ques1[i], test_ques2[i])\n",
    "    feature[2] = sentence_length_diff(test_ques1[i], test_ques2[i])\n",
    "    feature[3] = dup_words_diff(test_ques1[i], test_ques2[i])\n",
    "    feature[4] = oov_words_diff(test_ques1[i], test_ques2[i])\n",
    "    feature[5] = syllable_count_diff(test_ques1[i], test_ques2[i])\n",
    "    feature[6] = lexicon_count_diff(test_ques1[i], test_ques2[i])\n",
    "    test_feature_df.loc[len(test_feature_df)] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------GRAPH FEATURES-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>TOTAL NUMBER OF UNIQUE QUESTIONS =  4321567\n"
     ]
    }
   ],
   "source": [
    "q_dict = create_hash(train_df, test_df)\n",
    "train_df = get_hash(train_df, q_dict)\n",
    "test_df = get_hash(test_df, q_dict)\n",
    "print(\">>>>>>>TOTAL NUMBER OF UNIQUE QUESTIONS = \", len(q_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:   0%|                                                                | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  11%|██████▏                                                 | 1/9 [00:23<03:07, 23.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  22%|████████████▍                                           | 2/9 [00:41<02:26, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  33%|██████████████████▋                                     | 3/9 [00:59<01:59, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  44%|████████████████████████▉                               | 4/9 [01:13<01:31, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  56%|███████████████████████████████                         | 5/9 [01:34<01:15, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  67%|█████████████████████████████████████▎                  | 6/9 [01:52<00:56, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  78%|███████████████████████████████████████████▌            | 7/9 [02:10<00:37, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "CALCULATING KCORE FEATURES:  89%|█████████████████████████████████████████████████▊      | 8/9 [02:23<00:17, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CALCULATING KCORE FEATURES: 100%|████████████████████████████████████████████████████████| 9/9 [02:45<00:00, 18.34s/it]\n"
     ]
    }
   ],
   "source": [
    "k_dict = dict_from_hash(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = kcore_from_dict(train_df, k_dict)\n",
    "test_df = kcore_from_dict(test_df, k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = to_minmax(train_df, \"k_q\")\n",
    "test_df = to_minmax(test_df, \"k_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = get_neighbors(train_df, test_df)\n",
    "train_df = neighbor_similarity(train_df, neighbors)\n",
    "test_df = neighbor_similarity(test_df, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_map = dict(zip(*np.unique(np.vstack((df_total['qid1'], df_total['qid2'])), return_counts=True)))\n",
    "train_df = freq_features(train_df, f_map)\n",
    "test_df = freq_features(test_df, f_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = to_minmax(train_df, 'freq')\n",
    "test_df = to_minmax(test_df, 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feature_df['min_kcore'] = test_df['min_k_q']\n",
    "test_feature_df['max_kcore'] = test_df['max_k_q']\n",
    "test_feature_df['similar_neighbors'] = test_df['similar_neighbors']\n",
    "test_feature_df['similar_neighbor_ratio'] = test_df['similar_neighbor_ratio']\n",
    "test_feature_df['min_freq'] = test_df['min_freq']\n",
    "test_feature_df['max_freq'] = test_df['max_freq']\n",
    "del test_df\n",
    "test_feature_df.to_pickle('./Features/Train/Stat_Graph_features', compression='gzip')\n",
    "del test_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_df['min_kcore'] = train_df['min_k_q']\n",
    "train_feature_df['max_kcore'] = train_df['max_k_q']\n",
    "train_feature_df['similar_neighbors'] = train_df['similar_neighbors']\n",
    "train_feature_df['similar_neighbor_ratio'] = train_df['similar_neighbor_ratio']\n",
    "train_feature_df['min_freq'] = train_df['min_freq']\n",
    "train_feature_df['max_freq'] = train_df['max_freq']\n",
    "del train_df\n",
    "train_feature_df.to_pickle('./Features/Train/Stat_Graph_features', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
