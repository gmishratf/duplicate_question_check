{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply, subtract\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = 'dhainchu'\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 1025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open('../Data/glove.840B.300d.txt', encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif new_memento:\n",
    "            new_q = ['dhainchu'] + new_q\n",
    "            new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = ' '.join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "    for i, (q1, q2) in enumerate(list(zip(df['question1'], df['question2']))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/train_cleaned.csv\")\n",
    "test = pd.read_csv(\"../Data/test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question1'] = train['question1'].fillna(\"dhainchu\")\n",
    "train['question2'] = train['question2'].fillna(\"dhainchu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(train['question1'].tolist() + train['question2'].tolist()).unique()\n",
    "cv = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "cv.fit(all_questions)\n",
    "top_words = set(cv.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are not found in the embedding: {'what’s', '(in', 'quorans', '(i.e', 'demonetisation', 'movie):', '(for', '(the', '(a', '(as', '(2016', '(not', 'kvpy', 'brexit', '\"you', '(i', '(if', 'americaa', '\"a', '\"the', '(or', 'paytm', 'don’t', '(with', 'oneplus', '100%', '(like', 'etc)', 'india:', 'redmi', 'americaes', 'americae', 'series):', 'etc.)', '.net', '(tv', '(e.g', 'dhainchu', 'c#', 'i’m', '(and', 'movie)', '\"i', 'you\"', 'better:'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train questions are being prepared for LSTM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "q1s_train, q2s_train, train_q_features = extract_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle('../Features/Train/train_features', compression='gzip')\n",
    "train_features.drop('is_duplicate', axis=1, inplace=True)\n",
    "features_train = np.hstack((train_q_features, train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras import optimizers\n",
    "nadam = optimizers.Nadam(clipnorm=0.05, lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgseed = np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "[0]\tvalidation_0-logloss:0.641943\n",
      "[1]\tvalidation_0-logloss:0.599706\n",
      "[2]\tvalidation_0-logloss:0.563371\n",
      "[3]\tvalidation_0-logloss:0.533415\n",
      "[4]\tvalidation_0-logloss:0.507401\n",
      "[5]\tvalidation_0-logloss:0.485223\n",
      "[6]\tvalidation_0-logloss:0.466552\n",
      "[7]\tvalidation_0-logloss:0.450344\n",
      "[8]\tvalidation_0-logloss:0.435676\n",
      "[9]\tvalidation_0-logloss:0.422758\n",
      "[10]\tvalidation_0-logloss:0.411879\n",
      "[11]\tvalidation_0-logloss:0.401991\n",
      "[12]\tvalidation_0-logloss:0.39296\n",
      "[13]\tvalidation_0-logloss:0.384893\n",
      "[14]\tvalidation_0-logloss:0.37755\n",
      "[15]\tvalidation_0-logloss:0.371598\n",
      "[16]\tvalidation_0-logloss:0.365949\n",
      "[17]\tvalidation_0-logloss:0.361241\n",
      "[18]\tvalidation_0-logloss:0.357022\n",
      "[19]\tvalidation_0-logloss:0.353417\n",
      "[20]\tvalidation_0-logloss:0.350009\n",
      "[21]\tvalidation_0-logloss:0.346823\n",
      "[22]\tvalidation_0-logloss:0.343817\n",
      "[23]\tvalidation_0-logloss:0.341211\n",
      "[24]\tvalidation_0-logloss:0.338802\n",
      "[25]\tvalidation_0-logloss:0.33664\n",
      "[26]\tvalidation_0-logloss:0.334068\n",
      "[27]\tvalidation_0-logloss:0.332297\n",
      "[28]\tvalidation_0-logloss:0.330624\n",
      "[29]\tvalidation_0-logloss:0.328653\n",
      "[30]\tvalidation_0-logloss:0.326967\n",
      "[31]\tvalidation_0-logloss:0.325694\n",
      "[32]\tvalidation_0-logloss:0.323988\n",
      "[33]\tvalidation_0-logloss:0.322198\n",
      "[34]\tvalidation_0-logloss:0.321082\n",
      "[35]\tvalidation_0-logloss:0.319877\n",
      "[36]\tvalidation_0-logloss:0.318546\n",
      "[37]\tvalidation_0-logloss:0.317489\n",
      "[38]\tvalidation_0-logloss:0.316528\n",
      "[39]\tvalidation_0-logloss:0.315517\n",
      "[40]\tvalidation_0-logloss:0.314469\n",
      "[41]\tvalidation_0-logloss:0.313317\n",
      "[42]\tvalidation_0-logloss:0.312473\n",
      "[43]\tvalidation_0-logloss:0.311694\n",
      "[44]\tvalidation_0-logloss:0.310799\n",
      "[45]\tvalidation_0-logloss:0.309975\n",
      "[46]\tvalidation_0-logloss:0.308984\n",
      "[47]\tvalidation_0-logloss:0.308385\n",
      "[48]\tvalidation_0-logloss:0.307683\n",
      "[49]\tvalidation_0-logloss:0.307053\n",
      "[50]\tvalidation_0-logloss:0.306409\n",
      "[51]\tvalidation_0-logloss:0.305682\n",
      "[52]\tvalidation_0-logloss:0.305148\n",
      "[53]\tvalidation_0-logloss:0.30473\n",
      "[54]\tvalidation_0-logloss:0.304087\n",
      "[55]\tvalidation_0-logloss:0.303651\n",
      "[56]\tvalidation_0-logloss:0.303134\n",
      "[57]\tvalidation_0-logloss:0.302596\n",
      "[58]\tvalidation_0-logloss:0.301977\n",
      "[59]\tvalidation_0-logloss:0.30134\n",
      "[60]\tvalidation_0-logloss:0.300937\n",
      "[61]\tvalidation_0-logloss:0.300529\n",
      "[62]\tvalidation_0-logloss:0.300054\n",
      "[63]\tvalidation_0-logloss:0.299654\n",
      "[64]\tvalidation_0-logloss:0.299236\n",
      "[65]\tvalidation_0-logloss:0.298743\n",
      "[66]\tvalidation_0-logloss:0.298404\n",
      "[67]\tvalidation_0-logloss:0.298181\n",
      "[68]\tvalidation_0-logloss:0.297852\n",
      "[69]\tvalidation_0-logloss:0.29755\n",
      "[70]\tvalidation_0-logloss:0.296922\n",
      "[71]\tvalidation_0-logloss:0.296635\n",
      "[72]\tvalidation_0-logloss:0.2964\n",
      "[73]\tvalidation_0-logloss:0.295958\n",
      "[74]\tvalidation_0-logloss:0.29547\n",
      "[75]\tvalidation_0-logloss:0.295252\n",
      "[76]\tvalidation_0-logloss:0.294921\n",
      "[77]\tvalidation_0-logloss:0.29457\n",
      "[78]\tvalidation_0-logloss:0.294213\n",
      "[79]\tvalidation_0-logloss:0.293805\n",
      "[80]\tvalidation_0-logloss:0.293593\n",
      "[81]\tvalidation_0-logloss:0.293422\n",
      "[82]\tvalidation_0-logloss:0.293109\n",
      "[83]\tvalidation_0-logloss:0.292796\n",
      "[84]\tvalidation_0-logloss:0.292514\n",
      "[85]\tvalidation_0-logloss:0.292038\n",
      "[86]\tvalidation_0-logloss:0.29179\n",
      "[87]\tvalidation_0-logloss:0.291409\n",
      "[88]\tvalidation_0-logloss:0.291211\n",
      "[89]\tvalidation_0-logloss:0.291024\n",
      "[90]\tvalidation_0-logloss:0.290782\n",
      "[91]\tvalidation_0-logloss:0.290631\n",
      "[92]\tvalidation_0-logloss:0.290444\n",
      "[93]\tvalidation_0-logloss:0.290191\n",
      "[94]\tvalidation_0-logloss:0.289985\n",
      "[95]\tvalidation_0-logloss:0.28988\n",
      "[96]\tvalidation_0-logloss:0.289745\n",
      "[97]\tvalidation_0-logloss:0.28942\n",
      "[98]\tvalidation_0-logloss:0.289195\n",
      "[99]\tvalidation_0-logloss:0.289056\n"
     ]
    }
   ],
   "source": [
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "    \n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "    \n",
    "    xgmodel = xgboost.XGBClassifier(gpu_id=0, max_bin=16, tree_method='gpu_hist')\n",
    "    xg_eval_set = [(f_val, labels_val)]\n",
    "    xgmodel.fit(f_train, labels_train, eval_metric='logloss', eval_set=xg_eval_set, verbose=True)\n",
    "    xg_pred_train = xgmodel.predict(f_train)\n",
    "    xg_pred_val = xgmodel.predict(f_val)\n",
    "    \n",
    "    \n",
    "    print('Creating embedding layer')\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.1)\n",
    "\n",
    "    print('Creating input sequences')\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    print('creating embedding sequences')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    print('Passing embeddings to lstm')\n",
    "    x1 = lstm_layer(embedded_sequences_1)    \n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(1,), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(400, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.4)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.1)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=nadam,  metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "\n",
    "    print('Model checkpoint layer declaration')\n",
    "    #with tf.device('/cpu:0'):\n",
    "        #x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)#(x)\n",
    "    \n",
    "    print('Fitting')\n",
    "    #with tf.device('/CPU:0'):\n",
    "    hist = model.fit([data_1_train, data_2_train, xg_pred_train], labels_train,\n",
    "                         validation_data=([data_1_val, data_2_val, xg_pred_val], labels_val),\n",
    "                         epochs=15, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                         callbacks=[early_stopping, model_checkpoint], verbose=1) \n",
    "    \n",
    "    \n",
    "    print(\"Saving model\")\n",
    "    #with tf.device('/cpu:0'):\n",
    "        #model.save_weights('model_{}.h5'.format(model_count))\n",
    "    #export CUDA_VISIBLE_DEVICES=\"\"\n",
    "    model.load_weights(best_model_path)\n",
    "    print(\"Crash test\")\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "    \n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
