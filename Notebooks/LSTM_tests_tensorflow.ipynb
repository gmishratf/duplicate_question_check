{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply, subtract\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = 'dhainchu'\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 1025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open('../Data/glove.840B.300d.txt', encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif new_memento:\n",
    "            new_q = ['dhainchu'] + new_q\n",
    "            new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = ' '.join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "    for i, (q1, q2) in enumerate(list(zip(df['question1'], df['question2']))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/train_cleaned.csv\")\n",
    "test = pd.read_csv(\"../Data/test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question1'] = train['question1'].fillna(\"dhainchu\")\n",
    "train['question2'] = train['question2'].fillna(\"dhainchu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(train['question1'].tolist() + train['question2'].tolist()).unique()\n",
    "cv = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "cv.fit(all_questions)\n",
    "top_words = set(cv.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are not found in the embedding: {'(and', '\"the', 'brexit', '\"you', '100%', '(in', 'c#', 'etc)', '.net', '(for', '(a', 'better:', 'americaa', 'redmi', 'what’s', '\"i', '(with', 'paytm', 'americaes', 'americae', 'series):', 'i’m', '(like', '(tv', '(i.e', '(i', '(2016', 'oneplus', '(as', '(e.g', '(not', '(the', 'movie):', 'etc.)', 'demonetisation', 'quorans', '(or', 'you\"', '\"a', 'dhainchu', '(if', 'movie)', 'don’t', 'india:', 'kvpy'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train questions are being prepared for LSTM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "q1s_train, q2s_train, train_q_features = extract_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle('../Features/Train/train_features', compression='gzip')\n",
    "train_features.drop('is_duplicate', axis=1, inplace=True)\n",
    "features_train = np.hstack((train_q_features, train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras import optimizers\n",
    "nadam = optimizers.Nadam(clipnorm=0.05, lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgseed = np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "    \n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "    \n",
    "    xgmodel = xgboost.XGBClassifier(gpu_id=0, max_bin=16, tree_method='gpu_hist')\n",
    "    xg_eval_set = [(f_val, labels_val)]\n",
    "    xgmodel.fit(f_train, labels_train, eval_metric='logloss', eval_set=xg_eval_set, verbose=True)\n",
    "    xg_pred_train = xgmodel.predict(f_train)\n",
    "    xg_pred_val = xgmodel.predict(f_val)\n",
    "    \n",
    "    \n",
    "    print('Creating embedding layer')\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.1)\n",
    "\n",
    "    print('Creating input sequences')\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    print('creating embedding sequences')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    print('Passing embeddings to lstm')\n",
    "    x1 = lstm_layer(embedded_sequences_1)    \n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(1,), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(400, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.4)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.1)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='nadam',  metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "\n",
    "    print('Model checkpoint layer declaration')\n",
    "    #with tf.device('/cpu:0'):\n",
    "        #x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)#(x)\n",
    "    \n",
    "    print('Fitting')\n",
    "    #with tf.device('/CPU:0'):\n",
    "    hist = model.fit([data_1_train, data_2_train, xg_pred_train], labels_train,\n",
    "                         validation_data=([data_1_val, data_2_val, xg_pred_val], labels_val),\n",
    "                         epochs=15, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                         callbacks=[early_stopping, model_checkpoint], verbose=1) \n",
    "    \n",
    "    \n",
    "    print(\"Saving model\")\n",
    "    #with tf.device('/cpu:0'):\n",
    "        #model.save_weights('model_{}.h5'.format(model_count))\n",
    "    #export CUDA_VISIBLE_DEVICES=\"\"\n",
    "    model.load_weights(best_model_path)\n",
    "    print(\"Crash test\")\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "    \n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
